# Reservation Processor Service

Kubernetes-based replacement for the Lambda reservation processor.

## âš ï¸ CRITICAL: OpenTofu Only - NEVER Use Terraform

**ğŸš¨ THIS PROJECT USES OPENTOFU (tofu) EXCLUSIVELY ğŸš¨**

```bash
# âœ… CORRECT - Always use tofu
tofu init
tofu plan
tofu apply
tofu destroy

# âŒ WRONG - NEVER use terraform
terraform apply   # â›” DON'T DO THIS
terraform plan    # â›” DON'T DO THIS
terraform destroy # â›” DON'T DO THIS
```

**Why this matters:**
- ğŸ”’ **State file incompatibility**: Terraform and OpenTofu have different state formats
- ğŸ’¥ **Risk of infrastructure corruption**: Using terraform can corrupt the state
- ğŸ”„ **Version drift**: OpenTofu and Terraform diverged at 1.6.x
- ğŸ› **Unpredictable behavior**: Mixing tools will cause deployment failures

**Before running ANY command:**
1. âœ… Verify you're using `tofu`: `which tofu`
2. âœ… Check aliases: `alias | grep terraform`
3. âŒ If `terraform` is aliased to `tofu`, remove the alias - it's dangerous!

**Safety check:**
```bash
# Make sure tofu is installed
tofu version

# Make sure you're NOT accidentally using terraform
terraform version 2>&1 | grep -i "not found" && echo "âœ… Safe - terraform not in PATH"
```

## Architecture

- **Container**: Python 3.11 with psycopg2, boto3, kubernetes client, and pgmq
- **Deployment**: Kubernetes Deployment (runs continuously)
- **Queue**: PGMQ (postgres message queue)
- **Database**: PostgreSQL in controlplane namespace

## Directory Structure

```
terraform-gpu-devservers/
â”œâ”€â”€ shared/                        # Shared utilities (top-level)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ k8s_client.py             # Kubernetes client setup
â”‚   â”œâ”€â”€ k8s_resource_tracker.py   # GPU resource tracking
â”‚   â”œâ”€â”€ snapshot_utils.py         # EBS snapshot management
â”‚   â”œâ”€â”€ dns_utils.py              # Route53 DNS management
â”‚   â””â”€â”€ alb_utils.py              # ALB/NLB management
â””â”€â”€ reservation-processor-service/
    â”œâ”€â”€ Dockerfile                # Container image definition
    â”œâ”€â”€ requirements.txt          # Python dependencies (all-in-one)
    â””â”€â”€ processor/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py               # Main processing loop (PGMQ polling)
        â”œâ”€â”€ reservation_handler.py # Lambda handler logic (to be migrated)
        â””â”€â”€ buildkit_job.py       # BuildKit job creation utilities
```

**Note:** The `shared/` directory is at the top level of `terraform-gpu-devservers/` to allow sharing across multiple services (reservation processor, API service, etc.).

## Processing Flow

1. Service polls PGMQ queue `gpu_reservations` every 5 seconds
2. Retrieves messages with 5-minute visibility timeout
3. Processes reservation requests (creates pods, manages volumes, etc.)
4. On success: deletes message from queue
5. On failure: archives message for debugging

## Migration Status

### âœ… Completed
- Basic service structure with PGMQ polling
- Docker container setup
- Kubernetes deployment configuration
- IAM permissions (IRSA) for AWS resources
- Copied lambda code to new structure:
  - `reservation_handler.py` (7915 lines of lambda logic)
  - `buildkit_job.py` (buildkit job creation)
  - All shared utilities (k8s_client, snapshot_utils, dns_utils, alb_utils, k8s_resource_tracker)

### ğŸš§ TODO
- [ ] Replace SQS calls with PGMQ operations in `reservation_handler.py`
- [ ] Replace DynamoDB calls with PostgreSQL queries
- [ ] Update imports in `reservation_handler.py` to use new structure
- [ ] Integrate `reservation_handler.py` logic into `main.py`
- [ ] Test message processing end-to-end
- [ ] Add health checks and monitoring
- [ ] Performance tuning and optimization

## Environment Variables

- `POSTGRES_HOST` - PostgreSQL host (default: postgres-primary.controlplane.svc.cluster.local)
- `POSTGRES_PORT` - PostgreSQL port (default: 5432)
- `POSTGRES_USER` - Database user (default: gpudev)
- `POSTGRES_PASSWORD` - Database password (from secret)
- `POSTGRES_DB` - Database name (default: gpudev)
- `QUEUE_NAME` - PGMQ queue name (default: gpu_reservations)
- `POLL_INTERVAL_SECONDS` - Polling interval (default: 5)
- `VISIBILITY_TIMEOUT_SECONDS` - Message visibility timeout (default: 300)
- `BATCH_SIZE` - Number of messages to fetch per poll (default: 1)
- `AWS_REGION` - AWS region
- `EKS_CLUSTER_NAME` - EKS cluster name

## AWS Permissions (via IRSA)

The service has IAM permissions for:
- **STS**: GetCallerIdentity (for K8s auth)
- **EKS**: DescribeCluster
- **EC2**: Volume and snapshot management
- **ECR**: Docker image operations for buildkit

## Deployment

### Full Deployment (Recommended)

Deploy everything including Docker image build:
```bash
cd terraform-gpu-devservers
tofu apply -auto-approve
```

### Deploy Only Processor Image (After Code Changes)

If you've only changed the processor code and want to rebuild/redeploy just the image:
```bash
cd terraform-gpu-devservers
tofu apply -target=null_resource.reservation_processor_image
```

**âš ï¸ IMPORTANT: Always use `tofu apply` - NEVER manually build/push Docker images**

**âŒ WRONG - Don't do this:**
```bash
# DON'T: Manual build and push will fail if ECR doesn't exist
docker build -t reservation-processor:latest .
docker push $ACCOUNT_ID.dkr.ecr.us-east-2.amazonaws.com/reservation-processor:latest
```

**âœ… CORRECT - Use OpenTofu:**
```bash
# Correct: Handles everything automatically
tofu apply -target=null_resource.reservation_processor_image
```

**Why this matters:**
- âœ… ECR repository must exist before pushing (created by tofu)
- âœ… Proper build context from parent directory
- âœ… Automatic ECR authentication
- âœ… Triggers Kubernetes rollout
- âœ… Idempotent and safe

### Check Deployment Status

```bash
# Check pod status
kubectl get deployment -n gpu-controlplane reservation-processor

# View logs
kubectl logs -n gpu-controlplane -l app=reservation-processor -f

# Check rollout status
kubectl rollout status -n gpu-controlplane deployment/reservation-processor
```

## Development

### Local Testing
```bash
# Build container locally
cd reservation-processor-service
docker build -t reservation-processor:local .

# Run with local postgres
docker run --rm \
  -e POSTGRES_HOST=host.docker.internal \
  -e POSTGRES_PASSWORD=yourpassword \
  reservation-processor:local
```

### Code Organization

- **main.py**: Entry point, handles PGMQ polling and message routing
- **reservation_handler.py**: Original lambda handler logic (needs migration)
- **buildkit_job.py**: BuildKit job creation for Dockerfile builds
- **shared/**: Utilities shared with other services (K8s, AWS, DNS, etc.)

## Migration Notes

### SQS â†’ PGMQ Mapping
- `sqs_client.receive_message()` â†’ `pgmq.read()`
- `sqs_client.delete_message()` â†’ `pgmq.delete()`
- Message format: SQS JSON body â†’ PGMQ JSONB message column

### DynamoDB â†’ PostgreSQL Mapping
- `reservations` table â†’ `reservations` table (already exists)
- `disks` table â†’ `disks` table (already exists)
- `availability` table â†’ `gpu_availability` table (already exists)
- `dynamodb.Table().get_item()` â†’ `SELECT * FROM table WHERE ...`
- `dynamodb.Table().put_item()` â†’ `INSERT INTO table ...`
- `dynamodb.Table().update_item()` â†’ `UPDATE table SET ...`
- `dynamodb.Table().scan()` â†’ `SELECT * FROM table WHERE ...`

### Key Differences
1. **No Lambda context**: Remove `context` parameter usage
2. **Continuous running**: No cold starts, persistent connections
3. **Direct DB access**: No need for DynamoDB client setup
4. **PGMQ visibility timeout**: Automatic message redelivery on failure
